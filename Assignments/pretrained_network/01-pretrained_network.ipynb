{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained deep neural networks in PyTorch\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "This week, we will apply what we learned in the tutorials and get a quick idea of what a deep neural network is capable of when it comes to image classification tasks. To do so, we will play with a pre-trained neural network (ResNet101). \n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Pre-trained deep neural networks in PyTorch\n",
    "2. Making predictions using a neural network in Pytorch  \n",
    "    1. Defining a preprocess pipeline using PyTorch's transforms  \n",
    "    2. Loading and preprocessing data  \n",
    "    3. Making predictions using our neural network  \n",
    "    4. Interpreting the output  \n",
    "3. Playing with the ResNet model\n",
    "4. Good to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-trained deep neural networks in PyTorch\n",
    "\n",
    "As written in the documentation:\n",
    "\n",
    "> The [torchvision.models](https://pytorch.org/vision/stable/models.html#torchvision-models) subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification. \\[...\\] It provides pre-trained models.\n",
    "\n",
    "[ResNet](https://pytorch.org/vision/stable/models.html#id10) is a deep residual neural network that aims at classifying images. In Pytorch, several pre-trained ResNet models are available with different depths (resnet18, resnet34, resnet50, resnet101 and resnet152). Here we will use [resnet101](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet101).\n",
    "\n",
    "These pre-trained models were built and trained exactly as we did with our custom neural networks in the tutorials and can also be used in the exact same way. Unsurprisingly, they also subclass [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edvardsz/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/edvardsz/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /home/edvardsz/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8405b1fc80e74995aeeb9bfc5af0d079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/171M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch class of pre-trained  models:  <class 'torchvision.models.resnet.ResNet'>\n",
      "Which is subclass of a nn.Module:      True\n",
      "\n",
      " ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# The next line is all we need to create an instance of a pre-trained ResNet101 model \n",
    "# 101 means that we choose the ResNet architecture with 101 layers\n",
    "resnet = models.resnet101(pretrained=True)   \n",
    "print(\"Pytorch class of pre-trained  models: \", type(resnet))\n",
    "print(\"Which is subclass of a nn.Module:     \", issubclass(type(resnet), nn.Module))\n",
    "print(\"\\n\", resnet)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "1. If we have 1000 different labels (e.g cat, dog, mouse, goose, etc) what should be the dimension of the output layer of the neural network?\n",
    "1. In the output above we can see a module called \"Sequential\". We already met this module in the second and third tutorial, can you briefly explained what it is?\n",
    "1. In the output above we can also see a module called \"Bottleneck\". This module was very quickly mentioned in the third tutorial, do you remember what it is? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making predictions using a neural network in Pytorch\n",
    "\n",
    "In this section we will:\n",
    "\n",
    "1. Load an image and our labels\n",
    "1. Preprocess our image\n",
    "1. Make predictions using our neural network\n",
    "1. Interpret the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining a preprocess pipeline using PyTorch's transforms\n",
    "\n",
    "As we saw in the tutorials, the [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html#torchvision-transforms) module can easily performs the most common image transformations such as [Resize](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize), [CenterCrop](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.CenterCrop), [ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor), [Normalize](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize), etc. In addition, this module allows us to quickly define preprocessing pipelines using the [transforms.Compose](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Compose) method.\n",
    "\n",
    "In the following cell we define the pre-processing transformations that will be applied on our input images. Remember that when it comes to storing numerical data, the \"PyTorch-friendly objects\" are not numpy arrays but PyTorch's [tensors](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) and that the [ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor) transform implicitly:\n",
    "\n",
    "1. Reshapes a ``(H, W, C)`` image into a ``(C, H, W)`` tensor (Height, Width, Channel (color))\n",
    "2. Rescales ``[0 255]`` int arrays into ``[0 1]`` float tensors\n",
    "\n",
    "**TODO** \n",
    "\n",
    "Use [transforms.Compose](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Compose) as well as appropriate transforms in order to define a preprocessor ``preprocessor`` that:\n",
    "1. Resize images to ``256x256``  \n",
    "1. Crop images, keeping only the ``224x224`` pixels at the center\n",
    "1. Transform images to tensors\n",
    "1. Normalize tensors, using ``mean = [0.485, 0.456, 0.406]`` and ``std = [0.229, 0.224, 0.225]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "preprocessor = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our input batch:  torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Images\n",
    "# ------------------------------\n",
    "\n",
    "# Load one of our images\n",
    "img = Image.open(\"imgs/Bobby.jpeg\")\n",
    "# Preprocess our image using our preprocessor ('t' stands for 'tensor')\n",
    "img_t = preprocessor(img)\n",
    "# Reshape so that it is a batch (of size 1) as required in Pytorch         \n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "# Check that it has the required shape (N, C, H, W)\n",
    "# (See 2nd tutorial if you're struggling with shape conventions in Pytorch)\n",
    "print(\"Shape of our input batch: \", batch_t.size())\n",
    "\n",
    "# ------------------------------\n",
    "# Labels\n",
    "# ------------------------------\n",
    "\n",
    "# Read all the labels with which ResNet was trained and store them in the list 'labels'\n",
    "with open('list_labels.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Making predictions using our neural network\n",
    "\n",
    "After recalling that: \n",
    "\n",
    "> \"Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use [model.train()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train) or [model.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval) (from the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module)) as appropriate.\n",
    "\n",
    "We are now ready to make some predictions on our images. Let's show the output of the resnet model given our image of Bobby the Golden Retriever.\n",
    "\n",
    "**QUESTION** \n",
    "\n",
    "1. Set ``resnet`` in evaluation mode.\n",
    "1. Compute the output ``out`` corresponding to the input batch ``batch_t`` (defined in the cell above) \n",
    "1. Print the output tensor\n",
    "1. Print the dimension the output tensor using the [Tensor.size()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size) method\n",
    "2. Does it match your previous answer about the output dimension? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1882e+00,  6.4266e-01, -7.3113e-01, -1.1941e+00, -1.4031e+00,\n",
      "          1.6072e+00,  1.2287e+00, -1.8183e+00, -2.1664e+00, -2.6666e+00,\n",
      "         -1.6994e+00, -2.0940e+00, -2.2489e+00, -2.1980e+00, -2.0849e+00,\n",
      "         -2.7386e+00, -3.6760e+00, -5.5997e-01, -3.6721e-01, -8.9455e-01,\n",
      "         -1.7879e+00, -1.4200e+00,  2.4087e-01,  4.9729e-01,  5.8021e-01,\n",
      "         -3.8515e-01, -1.1698e+00, -6.4386e-01, -3.4965e-01, -5.2281e-01,\n",
      "         -2.4925e+00, -2.3248e+00, -1.2657e+00, -4.0828e-01, -7.6756e-01,\n",
      "         -1.9994e+00, -1.0509e+00, -1.7546e+00, -5.8461e-01, -1.4780e+00,\n",
      "         -1.3788e+00, -2.0631e+00,  1.6744e+00,  3.9316e-01, -1.7996e+00,\n",
      "         -7.8454e-01,  7.6017e-01, -6.1836e-01, -7.5581e-01, -2.1146e+00,\n",
      "         -1.8515e+00, -3.2559e-01, -1.2897e+00, -2.4264e+00, -1.5726e+00,\n",
      "         -4.0898e-01, -1.4480e+00, -1.9958e+00, -3.3056e-02, -8.0143e-01,\n",
      "         -5.9684e-02, -3.5059e-01, -7.4353e-01, -4.4914e-01, -5.9020e-02,\n",
      "          2.2616e-01, -1.3808e+00,  2.4145e-01, -1.4285e+00, -9.8448e-01,\n",
      "         -4.1575e-01,  3.5832e-01, -2.1845e+00, -1.2045e+00, -1.9407e+00,\n",
      "          5.5226e-01, -1.2495e+00,  3.1337e-01, -7.5730e-02,  2.1328e+00,\n",
      "         -2.7199e-01, -1.1814e+00,  2.8293e-01,  2.3480e+00, -4.9776e-01,\n",
      "         -2.1457e+00, -6.5659e-01,  1.0386e+00, -1.0902e+00, -9.1841e-01,\n",
      "         -2.4305e+00, -1.8880e+00, -3.9534e-01, -3.0212e+00, -4.7086e-01,\n",
      "         -2.8050e+00, -1.9975e+00, -1.1718e+00, -1.7560e+00, -4.4522e-01,\n",
      "         -3.1746e+00, -1.1018e+00, -2.2267e+00, -2.0492e+00, -2.2956e+00,\n",
      "         -4.5140e+00, -3.5408e+00,  3.0220e+00,  1.2657e+00, -5.6716e-01,\n",
      "         -1.6931e+00,  3.9531e-01,  1.0253e+00, -1.3143e+00, -2.8436e-02,\n",
      "         -1.2195e+00, -1.7119e+00, -6.0338e-02, -2.3336e+00, -2.5206e+00,\n",
      "         -2.4360e+00, -2.5174e+00, -1.0315e+00, -4.3451e-01,  1.0652e+00,\n",
      "          6.6497e-01,  7.8284e-01, -7.5954e-02, -2.7996e+00,  2.8881e-01,\n",
      "         -2.9048e+00, -1.4237e+00,  4.7661e-01, -3.4566e-01, -1.3858e+00,\n",
      "         -1.8710e+00, -2.3860e+00, -2.2443e+00, -1.3030e+00, -1.9382e+00,\n",
      "         -9.2964e-01, -3.0630e+00, -1.9357e+00, -2.3564e+00, -1.5365e+00,\n",
      "         -1.1186e+00, -1.1442e+00, -3.0481e+00, -2.4308e+00, -1.8379e+00,\n",
      "          3.0567e-01,  3.8865e+00,  3.0041e+00,  5.5938e+00,  6.5410e+00,\n",
      "          2.2759e+00,  2.4886e+00,  2.4331e+00,  3.3107e+00,  5.2189e+00,\n",
      "          5.6101e+00,  2.6224e+00,  3.9723e+00,  3.5202e+00,  3.0378e+00,\n",
      "          4.9266e+00,  2.9725e+00,  1.7690e+00,  8.1856e+00,  2.1722e+00,\n",
      "          2.4038e+00,  1.6452e+00,  2.7668e-01,  5.8056e-01,  1.8357e+00,\n",
      "          3.7701e+00,  6.1862e+00,  1.3287e+00,  3.2115e+00,  1.7904e+00,\n",
      "          2.6595e+00,  1.1759e+00,  1.8226e+00,  1.2012e-01,  2.9143e+00,\n",
      "          2.4543e+00,  6.5912e-01,  2.6387e+00, -7.3115e-01, -2.9252e-01,\n",
      "          5.8885e-01,  1.7685e+00,  2.0006e+00,  3.3328e+00,  4.5204e-01,\n",
      "          4.1179e-01,  7.1782e-01,  8.0707e-01, -1.4231e+00,  1.2868e+00,\n",
      "          3.1604e+00,  1.4591e+00,  1.3250e+00,  2.3978e+00,  4.1106e+00,\n",
      "          7.5197e+00,  4.0542e+00,  1.5126e+01,  1.1567e+01,  5.5420e+00,\n",
      "          2.3712e-01,  4.2621e+00,  6.1837e+00,  8.2170e+00,  6.6230e+00,\n",
      "          4.3736e+00,  4.9843e+00,  1.7238e+00,  2.7912e+00,  7.7970e+00,\n",
      "          5.6279e+00,  1.8041e+00,  7.2787e+00,  3.1905e+00,  3.4547e+00,\n",
      "          1.3446e+00,  2.9814e+00,  4.9794e+00,  1.1294e+00,  1.8431e+00,\n",
      "          4.3421e+00,  5.4213e+00,  4.0506e+00, -2.6857e+00,  4.2418e+00,\n",
      "          2.8740e+00,  2.9110e+00,  2.6679e+00,  3.3208e+00,  3.7229e+00,\n",
      "          2.4789e+00,  1.2825e+00,  2.1459e-01,  1.2488e+00,  4.5094e+00,\n",
      "         -1.7730e-01,  1.4313e+00,  1.9448e+00,  3.9924e+00,  2.4441e+00,\n",
      "          2.2824e+00,  4.1859e+00, -4.6544e-01,  1.0656e+00,  1.0006e+00,\n",
      "          5.6678e-01,  4.1051e+00,  7.9787e+00,  5.0100e+00,  4.8312e+00,\n",
      "          5.1936e+00,  1.8217e-01, -7.9374e-01,  4.5416e+00,  1.5074e+00,\n",
      "          4.0486e+00,  3.6292e+00,  3.3734e+00, -6.8242e-01,  1.4838e+00,\n",
      "          4.1579e+00, -1.0429e-01,  3.6596e-01,  4.7804e+00, -2.3931e-03,\n",
      "         -1.0770e+00,  1.3727e-01,  1.4234e-01, -1.8132e+00,  2.0430e+00,\n",
      "         -7.0253e-01,  9.3689e-01,  9.1534e-01,  2.6207e-01,  1.5092e+00,\n",
      "          9.3224e-01, -9.0593e-01, -1.5902e+00, -6.0562e-01, -3.2283e+00,\n",
      "         -1.7844e+00,  1.8469e+00, -1.7274e+00,  1.7868e+00,  1.4418e+00,\n",
      "         -9.3589e-01,  3.6166e+00, -1.1292e+00,  6.8997e-01,  1.8101e+00,\n",
      "         -3.7645e-01,  2.2271e+00, -1.0808e+00, -2.4286e+00,  9.4995e-01,\n",
      "          2.9345e-01, -4.5552e-02,  8.7948e-01,  1.2569e-01, -9.0149e-01,\n",
      "          4.4299e-01,  2.2569e-01,  4.9239e-01,  7.6210e-01,  3.2626e+00,\n",
      "         -1.0224e+00, -1.6425e+00,  1.8624e+00,  3.4747e-01,  1.4003e+00,\n",
      "         -6.4954e-01, -2.7135e+00, -1.8139e+00, -1.4727e+00, -4.6930e-01,\n",
      "         -1.4923e+00, -2.1408e+00,  1.2852e+00,  1.8792e+00, -1.8472e+00,\n",
      "         -1.3935e+00,  6.0851e-01,  2.5398e-01, -4.1844e-01,  4.9259e-01,\n",
      "         -1.4521e+00, -1.3569e+00, -3.9837e-01,  4.3990e-01, -3.1692e-01,\n",
      "         -1.0288e+00, -2.5128e+00, -2.7718e+00, -3.5194e+00, -3.2826e+00,\n",
      "         -2.3013e+00, -1.1694e+00, -2.7727e+00, -1.4751e+00, -3.7868e+00,\n",
      "         -1.5261e+00, -1.0472e+00, -4.4006e-01, -8.9210e-01, -1.3641e+00,\n",
      "         -1.4693e+00,  1.1996e+00, -3.9139e-01, -9.0481e-01,  6.1221e-01,\n",
      "          2.0752e-01, -5.0303e-01, -1.7757e-01, -4.6669e-01, -1.9965e+00,\n",
      "         -7.8797e-01, -4.0133e+00, -3.9865e+00, -1.7479e+00, -1.9184e+00,\n",
      "         -2.8020e+00, -3.2391e+00, -3.7383e+00, -2.0098e+00, -3.0892e-01,\n",
      "         -1.5393e+00, -1.8132e+00,  3.7660e-02, -1.8433e+00, -1.2654e+00,\n",
      "         -1.3500e+00, -3.7229e+00, -2.4363e+00, -3.8192e+00, -3.5714e+00,\n",
      "         -1.5244e+00,  1.7640e-01, -3.0686e+00, -2.7048e-01, -6.3789e-01,\n",
      "         -1.0837e+00, -2.3520e+00, -1.6036e+00, -1.6357e+00, -2.6875e+00,\n",
      "          9.2023e-01, -2.1254e+00,  1.2350e+00, -2.6149e+00,  1.3141e+00,\n",
      "          4.2233e-01,  1.0120e-01, -4.8544e-01, -3.2835e+00, -1.9717e+00,\n",
      "         -1.2538e+00,  7.2063e-01, -1.3411e+00, -8.8375e-01,  7.5390e-01,\n",
      "         -4.4573e+00,  2.4376e+00,  1.1553e-01, -3.2385e+00,  1.0090e+00,\n",
      "          6.5899e-01,  8.8603e-01,  2.2841e+00,  2.1820e+00,  2.3429e+00,\n",
      "         -1.2886e+00, -4.1246e-01, -1.8476e+00, -1.9064e+00, -1.9821e+00,\n",
      "         -1.3062e+00, -5.4134e-02, -9.5349e-01,  1.2603e-01,  4.9555e-01,\n",
      "         -1.4517e+00,  2.8162e+00,  9.6428e-01,  4.5420e+00,  7.5938e+00,\n",
      "          4.9000e+00, -1.6149e+00, -6.8568e-01, -1.5917e+00, -2.3403e+00,\n",
      "          1.0822e+00, -5.3618e-01, -5.0063e-01,  2.3844e+00, -3.6257e+00,\n",
      "          1.1131e+00,  1.7177e+00,  1.0860e-01,  1.2062e+00, -2.0677e+00,\n",
      "         -1.9723e+00,  5.7004e-01,  4.7533e+00, -7.6400e-01, -2.7342e+00,\n",
      "          1.2290e+00, -1.3473e+00,  2.4268e+00, -2.7622e+00,  1.2459e+00,\n",
      "         -3.3321e-01, -1.2537e+00,  1.3071e+00,  2.0138e+00, -1.3741e+00,\n",
      "          3.8799e-01, -5.4342e-01, -2.9521e+00, -2.5666e+00, -1.8461e+00,\n",
      "          2.3991e-01, -2.7430e+00, -7.7255e-01, -1.1408e+00,  1.5893e-01,\n",
      "         -2.1971e-01, -1.2364e+00, -2.1724e+00,  2.5317e+00, -5.8334e-01,\n",
      "         -1.3406e+00, -5.9886e-01, -2.2393e+00, -2.6819e+00, -1.2687e+00,\n",
      "         -2.1712e+00, -1.9608e+00,  7.1813e-01,  6.9527e-01, -7.6408e-02,\n",
      "         -6.2227e-01, -8.7142e-01,  6.5346e-01, -1.0304e-01, -1.2670e+00,\n",
      "          6.2389e-01,  2.5158e+00, -1.4163e+00, -1.1278e+00,  3.6019e-01,\n",
      "         -2.5900e+00,  9.6661e-01,  8.1727e-01, -1.0761e+00, -7.5221e-01,\n",
      "         -1.0457e+00, -5.0479e-01, -3.0215e+00,  6.4377e-01, -7.2001e-01,\n",
      "         -3.0732e+00,  2.3538e-01, -6.4266e-01, -1.6298e+00, -2.1313e-02,\n",
      "          3.9443e+00,  1.1851e+00,  2.5591e-03,  1.6867e+00,  1.9434e-01,\n",
      "          1.9817e+00, -1.8517e+00, -1.3369e-01,  3.3462e-01, -8.6371e-01,\n",
      "         -1.1249e-01, -1.7195e-01, -6.5850e-01, -7.5063e-01,  1.4334e+00,\n",
      "          6.0025e-01,  4.8783e-01,  1.2981e-01, -9.0629e-01,  3.5048e+00,\n",
      "         -2.2989e+00, -3.1280e+00,  1.4715e+00, -5.4550e-01,  3.6626e+00,\n",
      "         -2.3029e+00,  4.0837e-01,  2.4714e+00,  2.1704e+00, -3.2602e+00,\n",
      "          1.4575e+00, -2.2993e+00, -1.1074e+00, -1.4805e+00,  3.2411e+00,\n",
      "          2.4377e-01,  1.9165e+00,  6.7564e+00, -5.7771e-01, -9.7018e-01,\n",
      "         -3.1666e+00, -1.3052e+00, -4.2278e-01,  2.7692e+00,  1.3542e+00,\n",
      "         -1.8110e+00, -3.2092e+00, -1.7134e-01,  8.5180e-01, -8.5198e-02,\n",
      "         -2.9349e+00, -1.0107e+00,  1.9311e-01,  3.4914e+00, -1.9080e+00,\n",
      "         -1.1180e+00, -2.3670e+00,  1.1769e+00, -2.0821e+00,  2.0169e+00,\n",
      "          1.7867e-01, -2.4445e+00, -1.2681e+00,  4.7552e+00,  1.2198e+00,\n",
      "         -2.8544e-01, -3.5100e+00, -1.4739e+00, -1.4716e+00,  1.0415e+00,\n",
      "          1.1882e+00, -7.3846e-01, -3.3887e-01,  9.3625e-01,  5.2329e+00,\n",
      "         -2.1581e+00,  2.3973e+00, -2.7002e+00,  2.1020e-01,  8.6063e-02,\n",
      "         -7.6597e-01, -1.9462e+00, -1.5556e+00,  3.0481e-01, -2.1198e-01,\n",
      "         -7.6225e-01,  1.8659e+00, -2.7439e-01, -2.3300e+00,  2.5425e-01,\n",
      "         -3.2236e-02, -3.0466e-01, -5.0563e-01,  8.8015e-01, -2.4253e-01,\n",
      "          1.4876e+00, -9.6859e-01, -2.4244e+00, -5.9558e-01,  6.1899e-01,\n",
      "          2.2331e+00, -4.8163e-01,  1.1315e+00,  1.1715e+00,  2.2079e+00,\n",
      "          6.3012e-01, -5.1442e-01,  2.2490e+00,  8.0081e-01, -3.3131e+00,\n",
      "         -2.5791e+00, -7.0101e-01, -5.8864e-01, -3.2168e+00,  2.0340e+00,\n",
      "         -2.4115e+00,  1.0214e+00, -2.5526e+00, -1.3515e-01, -2.0505e+00,\n",
      "         -2.1321e+00, -9.9844e-01, -1.2092e+00,  4.5456e+00,  3.3774e+00,\n",
      "         -1.4027e+00,  2.7420e+00, -1.9157e+00,  1.7444e+00,  2.6059e-01,\n",
      "         -2.0282e+00, -4.2744e-01, -7.5971e-01,  1.1125e+00, -2.6670e+00,\n",
      "          2.2437e+00, -3.9567e-02, -3.1887e-01, -1.9116e+00, -2.2830e+00,\n",
      "          1.5484e+00, -1.0229e+00, -3.4595e-01,  9.9944e-01,  2.0037e+00,\n",
      "          1.3748e+00, -1.0900e+00, -2.1013e+00, -2.6711e+00, -6.2532e-02,\n",
      "         -2.8398e+00, -9.5080e-01,  2.9051e+00, -1.1188e-01,  2.1710e+00,\n",
      "         -1.2820e+00, -1.8556e+00,  5.8590e-01,  2.3800e+00, -1.6815e+00,\n",
      "         -3.7476e+00,  2.1195e+00,  6.9261e-01,  2.1225e+00, -6.2502e-01,\n",
      "          1.5035e+00,  6.2032e-01, -8.8668e-01,  1.8766e+00, -1.4376e+00,\n",
      "         -3.5182e+00, -1.0389e+00, -3.6392e+00, -2.9492e+00,  2.6579e+00,\n",
      "         -2.8438e+00,  4.5619e-01, -2.5960e-01,  1.7817e+00, -8.7776e-01,\n",
      "         -2.6741e+00,  2.6243e+00,  3.0177e+00, -1.1305e+00,  4.0378e-01,\n",
      "          5.2704e+00,  1.3930e+00, -1.0642e+00, -1.9793e+00, -1.4036e+00,\n",
      "          9.2455e-01, -7.8393e-01, -2.1442e+00, -1.2979e-01, -1.2875e+00,\n",
      "          1.5629e+00,  1.5949e+00,  4.5281e-02, -7.2649e-01, -2.5739e-01,\n",
      "         -1.0831e+00,  3.3820e-01, -1.1356e+00, -4.8475e-01,  6.7582e-01,\n",
      "         -1.0561e+00,  9.9619e-01,  2.4390e+00,  4.2740e-01,  5.4311e-01,\n",
      "         -1.1644e+00, -3.8871e+00, -9.8007e-01,  3.3297e+00,  5.2761e-02,\n",
      "         -2.4813e+00,  1.8637e+00, -7.0889e-01,  2.4339e+00, -2.8988e+00,\n",
      "          4.7255e+00,  1.9531e+00,  1.0668e+00, -1.5951e+00, -1.0789e+00,\n",
      "          1.2615e+00,  1.6101e+00,  1.8589e-01, -1.8182e+00, -1.1472e+00,\n",
      "         -1.6430e-01, -7.4582e-02, -7.8614e-01,  1.6348e-01,  2.2195e+00,\n",
      "          4.7509e+00, -3.6532e+00,  6.9422e-01, -4.8289e-01, -1.9079e+00,\n",
      "         -7.7214e-01, -2.4397e+00,  1.9666e+00, -4.8235e-01, -9.3929e-02,\n",
      "          8.7049e-01,  2.4422e+00, -1.7506e+00, -6.9958e-01, -2.3784e+00,\n",
      "          2.2014e+00, -2.4171e+00,  2.6908e+00, -2.2295e+00,  1.1383e+00,\n",
      "         -1.5270e+00, -1.8901e+00, -2.5334e-01,  1.3418e+00, -9.7594e-01,\n",
      "          2.2463e+00,  1.9483e+00, -3.1187e+00, -1.0312e+00,  2.1114e-01,\n",
      "          2.9582e-01, -2.8640e+00,  8.2376e-01, -5.4046e-01,  1.1142e+00,\n",
      "          3.3987e+00, -1.0696e+00, -2.2143e+00, -1.2008e+00,  1.9781e+00,\n",
      "         -1.0692e+00,  6.2022e-01,  9.9276e-01,  6.2723e+00,  3.9408e+00,\n",
      "         -1.8344e+00,  7.6096e-02,  3.6237e+00, -2.6380e+00,  2.0592e+00,\n",
      "         -2.8482e+00, -1.6667e+00, -1.7770e+00, -1.9848e+00,  2.0075e+00,\n",
      "          5.3174e+00,  4.1560e-01, -3.5459e+00,  3.2766e+00, -1.2666e+00,\n",
      "         -1.0854e+00,  1.8996e+00, -1.2333e+00,  3.8035e+00, -2.0562e+00,\n",
      "         -1.2600e+00,  2.8083e-01, -3.8196e+00,  8.4208e-01, -2.0003e-01,\n",
      "         -2.0030e+00, -3.8142e+00, -1.3756e+00,  1.5874e+00,  3.7245e+00,\n",
      "         -1.4731e+00,  1.0109e+00, -4.6690e-01,  3.1981e+00, -1.5554e+00,\n",
      "         -3.8999e+00,  1.1014e+00, -1.9669e+00, -1.4047e+00,  8.0343e-02,\n",
      "         -4.0111e+00, -3.6167e-01,  1.0188e+00,  4.3676e-01, -3.7258e+00,\n",
      "          9.3773e-01,  1.8184e+00,  8.6778e-01, -1.0025e+00,  3.1043e-01,\n",
      "          2.0035e+00,  6.3532e-01, -1.4469e+00, -1.9028e+00, -1.5702e+00,\n",
      "          3.0347e+00,  7.8725e-01,  7.6018e+00, -2.7251e+00, -1.6425e+00,\n",
      "          1.4856e+00, -1.9909e+00, -1.8864e+00, -1.5941e+00, -4.8435e-01,\n",
      "         -3.9888e+00,  1.4737e+00, -1.3532e-01, -1.8213e+00, -3.4725e+00,\n",
      "          9.3686e-02, -2.0352e-02, -8.3236e-01,  3.8450e-01,  2.1785e+00,\n",
      "         -1.4287e-01, -1.8810e+00,  7.2542e-01, -2.7266e+00, -2.7346e+00,\n",
      "         -7.3229e-01,  6.1846e+00, -2.9052e+00, -2.2589e+00,  3.9261e+00,\n",
      "         -4.9784e-01, -1.1850e+00,  1.9217e+00,  1.1739e+00, -7.5525e-01,\n",
      "          1.5813e+00, -1.9800e+00,  6.2219e-02, -2.5494e+00, -1.6103e+00,\n",
      "          4.3947e-02, -2.2542e+00,  2.5930e+00, -6.2955e-01, -1.0919e+00,\n",
      "         -1.4342e-01, -7.8868e-02,  1.1645e+00,  3.0971e+00,  6.8649e-01,\n",
      "         -4.8175e-01, -2.3848e+00,  5.9871e-01,  3.4729e+00,  2.4551e+00,\n",
      "          1.5585e+00,  4.0343e-01, -1.0885e+00,  5.2288e-01, -2.1843e+00,\n",
      "          3.0409e+00,  1.9974e+00, -1.7766e+00,  1.2357e+00, -3.0875e+00,\n",
      "         -5.0747e-01,  4.2020e-01, -2.8021e+00, -8.2161e-01, -1.9797e+00,\n",
      "         -1.1199e+00, -9.6368e-01, -2.3270e+00, -2.2567e-01, -2.1401e+00,\n",
      "         -1.5072e+00, -2.7759e+00, -1.1749e+00,  3.3787e-01,  3.9937e+00,\n",
      "         -1.3886e+00,  2.2978e+00,  4.0797e-01, -1.7986e+00, -1.8367e-01,\n",
      "          9.0234e-01,  1.2590e+00,  1.2773e+00, -1.1953e+00, -4.2547e-01,\n",
      "         -2.2120e+00, -3.0105e+00, -4.4624e-01, -3.0148e-01, -2.5320e+00,\n",
      "         -9.1011e-01, -1.5866e+00,  1.7384e+00,  2.0860e+00, -8.6716e-01,\n",
      "         -5.9609e-01, -5.1058e-01, -2.2455e+00, -8.2130e-01, -6.4579e-01,\n",
      "          3.8507e-02,  7.2858e-01, -2.2147e+00,  2.2498e-01, -8.8808e-01,\n",
      "         -1.3235e+00,  2.7667e+00, -1.8728e+00, -2.2777e+00, -6.3695e-01,\n",
      "         -1.5551e+00, -1.2564e+00, -1.1078e+00,  5.7871e-01,  2.1048e-01,\n",
      "         -6.6832e-01,  2.9189e+00, -1.9564e+00, -7.0883e-01, -1.5803e+00,\n",
      "          9.1994e-01, -1.2482e+00,  3.2351e+00,  7.8247e-01, -1.3590e+00,\n",
      "         -2.3530e+00, -8.3920e-01, -6.4745e-01, -2.2038e+00,  7.0312e-01,\n",
      "         -9.5580e-01, -2.9568e+00,  9.7844e-01, -3.8504e-01,  1.4968e-01,\n",
      "          2.6595e-01, -3.5390e+00,  3.8847e-01, -4.4667e+00, -2.8839e+00,\n",
      "          4.8044e-01, -1.5174e+00, -3.4601e+00,  2.0025e+00,  4.5668e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "resnet.eval()\n",
    "out = resnet(batch_t)\n",
    "print(out)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Interpreting the output\n",
    "\n",
    "You don't know what to do with that tensor right? How do you know if this output tensor means that the image is a dog or a cat or something else? \n",
    "\n",
    "Well that's actually simple. The first idea would be to find the most activated output unit, that is to say, the index of max value and find the label with the corresponding index. To do so we use the [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html?highlight=max#torch.max) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  tensor([207]) \n",
      "Label:  golden retriever \n",
      "Output value:  tensor([15.1264], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "_, index = torch.max(out, 1)\n",
    "print(\n",
    "    \"Index: \", index,  \n",
    "    \"\\nLabel: \", labels[index], \n",
    "    \"\\nOutput value: \", out[0, index]\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question is \"how to interpret this output value?\" How can we say if the model hesitates between this label and another one? \n",
    "\n",
    "We would like to convert this tensor value into something that could be interpreted as the confidence that the model has in its prediction. To do so, we use the [softmax](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.softmax) function which normalizes our outputs to \\[0, 1\\]\n",
    "For more information about the SoftMax function, you can watch the videos by Andrew Ng: \n",
    "- [Softmax Regression (C2W3L08)](https://www.youtube.com/watch?v=LLux1SW--oM)\n",
    "- [Training Softmax Classifier (C2W3L09)](https://www.youtube.com/watch?v=ueO_Ph0Pyqk)\n",
    "\n",
    "**QUESTION** \n",
    "\n",
    "1. Find the index corresponding to the max value of ``out`` **Hint:** Look at the previous cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  golden retriever \n",
      "Confidence:  96.42 %\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the index corresponding to the max value of out\n",
    "_, index = torch.max(out, 1)\n",
    "confidences = F.softmax(out, dim=1)[0]\n",
    "percentages = confidences * 100\n",
    "print(\n",
    "    \"Label: \",labels[index[0]], \n",
    "    \"\\nConfidence: \", round(percentages[index[0]].item(), 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-1 and Top-5 errors\n",
    "\n",
    "When evaluating an image classifier we often use the terms *Top-1 error* and *Top-5 error* \n",
    "\n",
    "If the classifier’s top guess is the correct answer (e.g., the highest score is for the “dog” class, and the test image is actually of a dog), then the correct answer is said to be in the Top-1. If the correct answer is at least among the classifier’s top 5 guesses, it is said to be in the Top-5.\n",
    "\n",
    "The top-1 score is the conventional accuracy, that is to say it checks if the top class (the one having the highest confidence) is the same as the target label. This is what we have done in the cell above. On the other hand, the top-5 score checks if the target label is one of your top 5 predictions (the 5 ones with the highest confidences). To do so we use the [torch.sort](https://pytorch.org/docs/stable/generated/torch.sort.html#torch-sort) function\n",
    "\n",
    "**QUESTIONS**\n",
    "\n",
    "1. Complete the code below **Hint:** Look at how we preprocessed the first image Bobby \n",
    "2. Does the model seem confident about the first prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess number  0 :  \n",
      "    Label:  golden retriever \n",
      "    Confidence:  88.52 %\n",
      "Guess number  1 :  \n",
      "    Label:  Great Pyrenees \n",
      "    Confidence:  8.31 %\n",
      "Guess number  2 :  \n",
      "    Label:  kuvasz \n",
      "    Confidence:  1.19 %\n",
      "Guess number  3 :  \n",
      "    Label:  chow, chow chow \n",
      "    Confidence:  0.95 %\n",
      "Guess number  4 :  \n",
      "    Label:  Newfoundland, Newfoundland dog \n",
      "    Confidence:  0.28 %\n"
     ]
    }
   ],
   "source": [
    "num_preds = 5\n",
    "\n",
    "img = Image.open(\"imgs/golden_retriever_online.jpeg\")\n",
    "# TODO: preprocess the image \n",
    "img_t = preprocessor(img)\n",
    "# TODO: create a batch of size 1\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "# TODO: Compute the output tensor of the tensor image contained in img_t\n",
    "out = resnet(batch_t)\n",
    "# TODO: Compute the percentage representing the confidence of the model about the output\n",
    "percentages = F.softmax(out, dim=1)[0] * 100\n",
    "_, indices = torch.sort(out, descending=True)\n",
    "\n",
    "results = [(labels[idx], round(percentages[idx].item(), 2)) for idx in indices[0][:num_preds]]\n",
    "for i_pred in range(num_preds):\n",
    "    print(\n",
    "        \"Guess number \", i_pred, \": \",\n",
    "        \"\\n    Label: \", results[i_pred][0], \n",
    "        \"\\n    Confidence: \",  results[i_pred][1],\"%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Playing with the ResNet model\n",
    "\n",
    "Put all the images that you want in the 'imgs/' folder (could be personal pictures or taken from the internet)\n",
    "\n",
    "**QUESTIONS**\n",
    "\n",
    "1. Complete the code below so that for each image it prints the 5 best guests according to the model\n",
    "2. When the image is a dog, what are usually the 1st, 2nd, 3rd guesses? \n",
    "3. Use one of your personal pictures of an object whose label is in the list of labels.\n",
    "4. Try to find an image on the web whose label is in the list of labels but whose corresponding prediction is wrong. How can you try to make it difficult for the model to recognize the object? \n",
    "5. Try to find an image on the web whose label is NOT in the list of labels with which the model was trained. Look at the output, is it consistent even though it is necessarily wrong? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======  seal_meme01.png  ====== \n",
      "Guess number  0 :  \n",
      "    Label:  Egyptian cat \n",
      "    Confidence:  12.74 %\n",
      "Guess number  1 :  \n",
      "    Label:  sea lion \n",
      "    Confidence:  9.52 %\n",
      "Guess number  2 :  \n",
      "    Label:  hare \n",
      "    Confidence:  7.4 %\n",
      "Guess number  3 :  \n",
      "    Label:  mosquito net \n",
      "    Confidence:  5.87 %\n",
      "Guess number  4 :  \n",
      "    Label:  mushroom \n",
      "    Confidence:  5.28 %\n",
      "\n",
      " ======  dog_meme.png  ====== \n",
      "Guess number  0 :  \n",
      "    Label:  Eskimo dog, husky \n",
      "    Confidence:  10.42 %\n",
      "Guess number  1 :  \n",
      "    Label:  Siberian husky \n",
      "    Confidence:  8.54 %\n",
      "Guess number  2 :  \n",
      "    Label:  dingo, warrigal, warragal, Canis dingo \n",
      "    Confidence:  5.42 %\n",
      "Guess number  3 :  \n",
      "    Label:  malamute, malemute, Alaskan malamute \n",
      "    Confidence:  4.08 %\n",
      "Guess number  4 :  \n",
      "    Label:  cougar, puma, catamount, mountain lion, painter, panther, Felis concolor \n",
      "    Confidence:  3.31 %\n",
      "\n",
      " ======  Akita_personal.jpg  ====== \n",
      "Guess number  0 :  \n",
      "    Label:  Samoyed, Samoyede \n",
      "    Confidence:  79.72 %\n",
      "Guess number  1 :  \n",
      "    Label:  Eskimo dog, husky \n",
      "    Confidence:  8.23 %\n",
      "Guess number  2 :  \n",
      "    Label:  white wolf, Arctic wolf, Canis lupus tundrarum \n",
      "    Confidence:  3.74 %\n",
      "Guess number  3 :  \n",
      "    Label:  Great Pyrenees \n",
      "    Confidence:  2.52 %\n",
      "Guess number  4 :  \n",
      "    Label:  Siberian husky \n",
      "    Confidence:  1.54 %\n",
      "\n",
      " ======  Bobby.jpeg  ====== \n",
      "Guess number  0 :  \n",
      "    Label:  golden retriever \n",
      "    Confidence:  96.42 %\n",
      "Guess number  1 :  \n",
      "    Label:  Labrador retriever \n",
      "    Confidence:  2.74 %\n",
      "Guess number  2 :  \n",
      "    Label:  Irish setter, red setter \n",
      "    Confidence:  0.1 %\n",
      "Guess number  3 :  \n",
      "    Label:  redbone \n",
      "    Confidence:  0.09 %\n",
      "Guess number  4 :  \n",
      "    Label:  Great Pyrenees \n",
      "    Confidence:  0.08 %\n",
      "\n",
      " ======  husky_meme.png  ====== \n",
      "Guess number  0 :  \n",
      "    Label:  Siberian husky \n",
      "    Confidence:  54.56 %\n",
      "Guess number  1 :  \n",
      "    Label:  Eskimo dog, husky \n",
      "    Confidence:  44.08 %\n",
      "Guess number  2 :  \n",
      "    Label:  malamute, malemute, Alaskan malamute \n",
      "    Confidence:  0.58 %\n",
      "Guess number  3 :  \n",
      "    Label:  dogsled, dog sled, dog sleigh \n",
      "    Confidence:  0.38 %\n",
      "Guess number  4 :  \n",
      "    Label:  white wolf, Arctic wolf, Canis lupus tundrarum \n",
      "    Confidence:  0.23 %\n",
      "\n",
      " ======  seal_meme02.png  ====== \n",
      "Guess number  0 :  \n",
      "    Label:  strainer \n",
      "    Confidence:  24.08 %\n",
      "Guess number  1 :  \n",
      "    Label:  Petri dish \n",
      "    Confidence:  18.39 %\n",
      "Guess number  2 :  \n",
      "    Label:  nipple \n",
      "    Confidence:  14.78 %\n",
      "Guess number  3 :  \n",
      "    Label:  electric fan, blower \n",
      "    Confidence:  3.18 %\n",
      "Guess number  4 :  \n",
      "    Label:  spotlight, spot \n",
      "    Confidence:  1.94 %\n",
      "\n",
      " ======  golden_retriever_online.jpeg  ====== \n",
      "Guess number  0 :  \n",
      "    Label:  golden retriever \n",
      "    Confidence:  88.52 %\n",
      "Guess number  1 :  \n",
      "    Label:  Great Pyrenees \n",
      "    Confidence:  8.31 %\n",
      "Guess number  2 :  \n",
      "    Label:  kuvasz \n",
      "    Confidence:  1.19 %\n",
      "Guess number  3 :  \n",
      "    Label:  chow, chow chow \n",
      "    Confidence:  0.95 %\n",
      "Guess number  4 :  \n",
      "    Label:  Newfoundland, Newfoundland dog \n",
      "    Confidence:  0.28 %\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Load inputs\n",
    "# ------------------------------\n",
    "\n",
    "# Load all the images in the 'imgs/' folder\n",
    "list_img_t = []                  # Where input tensors will be stored\n",
    "path_imgs = 'imgs/'   \n",
    "list_files = listdir('imgs/')    # Find all filenames in the 'imgs/' folder\n",
    "for f in list_files:\n",
    "    img = Image.open(path_imgs + f)\n",
    "    img = img.convert('RGB')  # Because some of the images are in the RGBA format while ResNet requires a RGB format\n",
    "\n",
    "    img_t = preprocessor(img)\n",
    "\n",
    "    list_img_t.append(torch.unsqueeze(img_t, 0) )\n",
    "\n",
    "# ------------------------------\n",
    "# Make predictions\n",
    "# ------------------------------\n",
    "num_preds = 5\n",
    "for i, batch_t in enumerate(list_img_t):\n",
    "    print(\"\\n ====== \", list_files[i], \" ====== \")\n",
    "\n",
    "    out = resnet(batch_t)\n",
    "    # TODO: Compute the percentage representing the confidence of the model about the output\n",
    "    percentages = F.softmax(out, dim=1)[0] * 100\n",
    "    # TODO: Sort the out tensor in descending order\n",
    "    _, indices = torch.sort(out, descending=True)\n",
    "    results = [(labels[idx], round(percentages[idx].item(), 2)) for idx in indices[0][:num_preds]]\n",
    "    for i_pred in range(num_preds):\n",
    "        print(\n",
    "            \"Guess number \", i_pred, \": \",\n",
    "            \"\\n    Label: \", results[i_pred][0], \n",
    "            \"\\n    Confidence: \",  results[i_pred][1],\"%\"\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Good to know\n",
    "- In PyTorch, data are stored in [tensors](https://pytorch.org/docs/stable/tensors.html#torch.Tensor). This is the Pytorch counterpart of Numpy's array and most of the methods that are available in Numpy are also available in Pytorch. (e.g \n",
    "[size](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size), \n",
    "[amax](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.amax), \n",
    "[argmax](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.argmax), \n",
    "[sort](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sort), \n",
    "[abs](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.abs), \n",
    "[cos](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cos), \n",
    "[sum](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sum) etc.)\n",
    "- In PyTorch all neural networks should be a class that is itself a subclass of the PyTorch's [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module) class\n",
    "- There are many well-known deep neural network architectures available in the [torchvision.models](https://pytorch.org/vision/stable/models.html?highlight=models) sub-package. \n",
    "  - For each of these architectures a pre-trained model is available. \n",
    "  - Some of them such as the ResNet architecture even have multiple pre-trained model instances of different depths. For the [ResNet](https://pytorch.org/vision/stable/models.html#id10) class, we have [resnet18](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet18), [resnet50](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet50), [resnet101](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet101), etc.\n",
    "- During the preprocessing, we can use the [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html#torchvision-transforms) module to perform the most common image transformations\n",
    "- Some models use modules that have different training and evaluation behavior, such as batch normalization. To switch between these modes, we use [model.train()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train) and [model.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval) accordingly\n",
    "- Top-1 and Top-5 scores are commonly used in image classification\n",
    "- When there are more than 2 possible classes we often use the [SoftMax]((https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.softmax)) function in the output layer to convert the output tensor values into confidence values.\n",
    "- However, we will see in this course that we don't need a softmax function in the output layer if we use [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentro#torch.nn.CrossEntropyLoss) loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "79938c2f5c21d2323d35c0a981ad2f62680a8580354cf190d7e2bff72c612276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
